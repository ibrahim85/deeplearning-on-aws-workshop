{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd를 포함한 필요한 라이브러리를 Import합니다. 이 예제는 Python 2 를 기반으로 작성되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "\n",
    "이 랩은 Linear regression만을 다룹니다. 입력 데이터 모음인 ``X``, 그리고 target value ``y``, vector ``w``, intercept ``b``입니다. w와 b를 가장 적절한 값으로 하여 ``X[i]`` 와 그 값의 레이블인 ``y[i]``를이용하여 우리는 회귀되는 직선을 구하고, 예측(prediction)에 사용합니다.\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}} = X \\cdot \\boldsymbol{w} + b$$\n",
    "\n",
    "squared error 최소화\n",
    "\n",
    "$$\\sum_{i=1}^n (\\hat{y}_i-y_i)^2.$$\n",
    "\n",
    "Linear regresion은 매우 오래된 모델이지만, Neural Network 을 이해하는 초석이되므로 실습을 통해 이해할 수 있도록합니다. 실제 Neural Network은 각각의 Edge를 통해 연결되며 각 노드(뉴런)의 값을 계산하기위해서는 실제 입력값의 weighted sum을 *activation fuction*을 적용합니다. Linear regression은 두 개의 Layer로 구성되는 것으로 이해할 수 있으며, 입력값이 아래 그름의 주황색 부분이고, 녹색의 부분은 Output node가 됩니다. activation function은 identity function과 같은 개념으로 볼 수 있습니다.\n",
    "\n",
    "그림에서 주황색 원이 입력값을 의미합니다.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zackchase/mxnet-the-straight-dope/master/img/simple-net-linear.png)\n",
    "\n",
    "쉽게 해보기 위해 우선 램덤값 ``X[i]``을 사용하여 우선 아래 식으로 진행합니다. 레이블은 ``y[i] = 2 * X[i][0]- 3.4 * X[i][1] + 4.2 + noise``으로하며, noise역시 랜덤값(0~1)을 사용합니다.\n",
    "\n",
    "$$y = X \\cdot w + b + \\eta, \\quad \\text{for } \\eta \\sim \\mathcal{N}(0,\\sigma^2)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_examples = 10000\n",
    "\n",
    "X = nd.random_normal(shape=(num_examples, num_inputs))\n",
    "y = 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2 + .01 * nd.random_normal(shape=(num_examples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each row in ``X`` consists of a 2-dimensional data point and that each row in ``Y`` consists of a 1-dimensional target value. \n",
    "\n",
    "실제 식이 의미하는 바는 2차원 값인 ``X``를 1차원 값의 ``Y``로 만들어냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "적절한 파라미터 값을 가진 함수는 어떠한 램던값이 주어져도 타겟값에 가까운 예측값을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(2 * X[0, 0] - 3.4 * X[0, 1] + 4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the correspondence between our second feature (``X[:, 1]``) and the target values ``Y`` by generating a scatter plot with the Python plotting package ``matplotlib``.\n",
    "\n",
    "파이썬 패키지인 matplotlib을 이용하여 우리는 시각화하여 식을 표현할 수 있습니다. (``X[:, 1]``) 와 ``Y`` 값을 scatter plot으로 보여줍니다.\n",
    "\n",
    "matplotlib이 없을 시 설치 --> ``pip2 install matplotlib`` (for Python 2) 또는 ``pip3 install matplotlib`` (for Python 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 1].asnumpy(),y.asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterators\n",
    "\n",
    "일단 Neural network을 사용하기위해서는, 빠르게 데이터를 반복적으로 입력할 수 있어야 합니다. 아래에서는 ``k``값을 배치값으로 지정하여 수행하며, 데이터를 섞는 작업을 포함합니다. 이 Interator는 빠르게 데이터를 가져와서 처리할 수 있는 방법을 제공하게되는데, ``NDArrayIter``가 클래스가 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(X, y),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDArrayIter (``train_data``)를 일단 초기화하면 ``train_data.next()``. ``batch.data``  를 호출하여 데이터 값을 가져옵니다. 예를 들어 처번째 Batch는 ``batch.data[0]``이됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for data, label in train_data:\n",
    "    print(data, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 ``train_data`` 에 list로 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for data, label in train_data:\n",
    "    counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "\n",
    "이제 파마미터를 설정하기 위한 메모리를 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "params = [w, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좀 더 알맞는 파라미터를 찾기위해 *loss function*인 gradient (a multi-dimensional derivative)이용합니다. loss를 줄이기위해 parameter을 업데이트하게 됩니다. 그 작업을 위해서는 우선 메모리가 필요하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "이제 다음으로 모델을 정의할 차례입니다. 앞서 언급된 대로 Linear 모델을 활용할 것입니다. 가장 단순하지만 매우 유용한 모델입니다. 간단히  weights (``w``)와 intercept ``b``를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return mx.nd.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "모델을 트래이닝 한다는 의미는 모델을 계속 정확도를 높히기위해 지속적으로 학습시킨는 것입니다. 우리는 squared distance 를 이용하여 예측값과 실제 값의 차이를 활용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_loss(yhat, y): \n",
    "    return nd.mean((yhat - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Linear regression은 제한된 셋을 안에서 움직입니다. 따라서 분석을 위한 모델로 바로 사용이 어렵습니다. 우리는 이러한 문제는 SGD (stochastic gradient descent)를 이용하여 해결합니다. 각 단계마다 특정 weight에서의 loss의 gradient를 예측합니다. 이는 가진 dataset에서 임의의 한 셋에대해서 배치를 실행하고 loss를 줄이기위한 방향으로 parameter를 업데이트합니다. *learning rate*에 따라서 단계의 크기는 달라집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop 실행\n",
    "\n",
    "이제 전체 학습을 위한 전체를 실행해 볼 수 있습니다. 먼저 ``epochs``를 정의합니다. 이것은 dataset에 반복할 횟수를 정의합니다. 각 한번의 수행마다 ``train_data``에서 데이터 배치에 대해 입력값과 레이블을 만듭니다.\n",
    "\n",
    "각 배치에서 반복적으로 아래 작업을 수행합니다.\n",
    "* Generate predictions (``yhat``) and the loss (``loss``) by executing a forward pass through the network.\n",
    "* Calculate gradients by making a backwards pass through the network (``loss.backward()``). \n",
    "* Update the model parameters by invoking our SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "ctx = mx.cpu()\n",
    "learning_rate = .001\n",
    "smoothing_constant = .01\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx).reshape((-1, 1))\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(\"Epoch %s, batch %s. Moving avg of loss: %s\" % (e, i, moving_loss))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "mxnet.ndarray와 mx.net.autograd를 활용하는 것을 해보았습니다. 간단히 모델을 구성할 수 있습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
