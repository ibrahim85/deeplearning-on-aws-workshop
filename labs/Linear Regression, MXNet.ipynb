{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본격적으로 시작하기에 앞서, 필요한 라이브러리인 autograd와 nd를 Import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "\n",
    "이번 실습은 linear regression을 다룹니다. 입력 데이터 포인트 ``X``와 해당하는 목표 값(target value) ``y``이 주어졌을 때, linear regression은 ``X``와 ``y`` 간의 관계를 가장 잘 표현할 수 있는 최적의 weight vector ``w``와 intercept (y 절편) ``b`` 값을 찾습니다. Linear regression의 결과로 얻는 prediction 식은 다음과 같이 표현됩니다.\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}} = X \\cdot \\boldsymbol{w} + b$$\n",
    "\n",
    "이때, 이 식은 다음과 같이 에러 제곱의 총합을 최소화합니다.\n",
    "\n",
    "$$\\sum_{i=1}^n (\\hat{y}_i-y_i)^2.$$\n",
    "\n",
    "Neural network를 배우는 과정에서 오래된 모델인 linear regression이 왜 등장했는지 궁금할 수도 있는데, 그 이유는 linear regression을 가장 단순(하고 유용)한 neural network로 표현할 수 있기 때문입니다. Neural network는 방향이 있는 에지로 연결된 노드(뉴런)들의 모음이며, 노드들은 여러 개의 계층(layer) 단위로 그룹화됩니다. 각 노드의 값은 입력 값들의 weighted sum을 *activation fuction*에 통과시켜 계산합니다.\n", 
    "\n",
    "Linear regression은 두 개의 layer로 만들 수 있는데, 아래 그림에서 오렌지 색 부분은 입력 노드이고 하늘색 부분은 출력 노드입니다. 이때 activiation function은 identity function입니다.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zackchase/mxnet-the-straight-dope/master/img/simple-net-linear.png)\n",
    "\n",
    "쉽게 해보기 위해, 이번 실습에서는 랜덤 데이터 ``X[i]``로부터 레이블 ``y[i]``를 ``y[i] = 2 * X[i][0]- 3.4 * X[i][1] + 4.2 + noise`` 라는 식으로 생성하겠습니다. 이때 noise는 평균 0, 분산 0.01의 분포를 따르는 Gaussian random variable입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_examples = 10000\n",
    "\n",
    "X = nd.random_normal(shape=(num_examples, num_inputs))\n",
    "y = 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2 + .01 * nd.random_normal(shape=(num_examples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 ``X``의 각 row는 2차원 데이터 포인트이며 ``Y``의 각 row는 1차원 목표 값입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤으로 주어진 데이터와 (이미 알고 있는) 최적의 파라미터들의 linear combination 값을 계산해보면 목표 값과 매우 근사한 예측치를 얻을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(2 * X[0, 0] - 3.4 * X[0, 1] + 4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬 패키지 ``matplotlib``를 사용해 두 번째 feature (``X[:, 1]``)와 목표 값 ``Y``의 상관 관계를 scatter plot으로 시각화해 볼 수 있습니다.\n", 
    "\n",
    "``matplotlib``이 설치되어 있지 않다면 다음 명령어로 설치하시기 바랍니다: ``pip2 install matplotlib`` (Python 2) 또는 ``pip3 install matplotlib`` (Python 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 1].asnumpy(),y.asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterators\n",
    "\n",
    "Neural network를 사용할 때, 데이터에 반복적으로 빠르게 접근하거나 k개의 데이터 포인트를 배치(batch) 단위로 한 번에 가져오고 데이터를 섞는(shuffle) 등의 다양한 작업이 필요합니다. MXNet에서는 data iterator가 데이터 가져오기와 조작을 위한 유틸리티를 제공합니다. 이번 실습에서는 ``NDArrayIter`` 클래스를 사용하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(X, y),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDArrayIter (``train_data``)를 초기화했다면, ``train_data.next()`` 를 호출해 다음 데이터 배치를 가져 옵니다. ``batch.data``를 호출하면 입력의 목록을 얻을 수 있는데, 우리의 모델에는 하나의 입력 (``X``)만 존재하므로 ``batch.data[0]``를 사용하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for data, label in train_data:\n",
    "    print(data, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로, ``train_data``에 대해 (일반적인 파이썬 list와 마찬가지 방법으로) 반복합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for data, label in train_data:\n",
    "    counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "\n",
    "이제 파라미터를 설정하기 위해 메모리를 할당하고 초기 값을 지정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "params = [w, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래에서, 우리가 가진 데이터에 더 잘 맞도록 이 파라미터들을 업데이트할 것입니다.
 이를 위해 *loss function*의 gradient를 계산한 다음, loss를 감소시키는 방향으로 파라미터를 업데이트합니다. 그 전에 먼저, 각 gradient를 위해 메모리를 할당하겠습니다.\n" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "이제 다음으로 모델을 정의합니다. 앞에서 언급한 것처럼 linear 모델을 만들텐데, 가장 단순하지만 유용한 neural network입니다. 간단히 입력에 weight (``w``)를 곱하고 offset ``b``를 더해서 linear 모델의 출력을 계산합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return mx.nd.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "모델을 학습시킨다는 것은 학습 중에 점점 더 좋아지는 것을 의미합니다. 하지만 그러기 위해서는 *좋아지는* 것이 무엇인지 먼저 정의할 필요가 있습니다. 이번 실습에서는 그 기준으로 예측 값과 실제 값의 차이의 제곱을 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_loss(yhat, y): \n",
    "    return nd.mean((yhat - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "사실 linear regression은 closed form solution을 가지고 있기 때문에, 굳이 gradient descent를 통해 점진적으로 최소값을 찾는 방식을 취하지 않아도 됩니다. 하지만, 대부분의 실제 문제는 해석적으로 답을 구할 수 없기 때문에 이번 실습에서는 stochastic gradient descent로 접근해 보겠습니다. 매 단계마다 입력 데이터셋에서 랜덤하게 추출한 한 배치의 데이터를 사용해 loss function의 gradient를 계산합니다. 다음으로, loss function이 감소하는 방향으로 파라미터를 업데이트합니다. 각 단계마다 업데이트를 반영하는 정도를 *learning rate* ``lr``로 결정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop 실행\n",
    "\n",
    "이제 학습하는 전체 loop을 만들어 앞에서 설명한 것들을 연결하기만 하면 됩니다. 먼저 데이터셋에서 반복할 횟수인 ``epochs``를 정의합니다. 다음으로, 각 반복마다 데이터의 한 배치인 ``train_data``에서 데이터와 레이블을 얻습니다.\n",
    "\n",
    "매번 각 배치에서 반복적으로 아래 작업을 수행합니다.\n",
    "* Neural network의 forward pass를 통해 예측 값 (``yhat``)과 (``loss``)를 계산합니다.\n",
    "* Neural network의 backward pass (``loss.backward()``)를 통해 gradient를 계산합니다.\n",
    "* SGD optimizer를 호출해 모델의 파라미터를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "ctx = mx.cpu()\n",
    "learning_rate = .001\n",
    "smoothing_constant = .01\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx).reshape((-1, 1))\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(\"Epoch %s, batch %s. Moving avg of loss: %s\" % (e, i, moving_loss))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "지금까지 mxnet.ndarray와 mxnet.autograd를 활용해 linear regression 모델을 처음부터 만들어 봤습니다. 이후 실습에서는 이를 기반으로 최신 neural network의 이론과 MXNet의 활용 방법을 소개하겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
