{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache MXNet Basic 실습\n",
    "## Symbol - Neural network graphs와 자동 미분(auto-differentiation)\n",
    "\n",
    "* Symbol은 이용한 선언형(declarative) 프로그래밍 방법은,\n",
    "\n",
    "    1. 입력과 출력에 대한 Placeholder을 포함한 연산 그래프 정의\n",
    "    2. NDArray 변수들을 Placeholder에 바인딩한 후, 그래프 연산을 계산\n",
    "\n",
    "\n",
    "* 최적화된 연산 수행 (예, 메모리 재사용을 통한 적은 메모리 사용)\n",
    "\n",
    "> 참조 사이트\n",
    ">\n",
    "> [MXNet How To](http://mxnet.io/how_to/index.html)\n",
    ">\n",
    "> [MXNet Architecture](http://mxnet.io/architecture/index.html)\n",
    ">\n",
    "> [a more thorough discussion on the comparative strengths of imperative and symbolic programing](http://mxnet.io/architecture/program_model.html)\n",
    "> \n",
    "> [Symbolic Configuration and Execution in Pictures](http://mxnet.io/api/python/symbol_in_pictures.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 Symbol 조합\n",
    "\n",
    "### <font color='red'>기본 연산들</font>\n",
    "\n",
    "아래 예제는 간단한 그래프 연산 `a + b` 을 정의합니다. 우선, `mx.sym.Variable`를 이용해서 placeholder 두개를 정의하고, `+` 연산부호로 원하는 symbol을 정의합니다. 각 placeholder에는 a,b라는 이름을 정하지만, 정하지 않을 경우에는 MXNet이 임의로 이름을 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "a = mx.sym.Variable('a')\n",
    "b = mx.sym.Variable('b')\n",
    "c = a + b\n",
    "(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NDArray`에서 지원하는 대부분의 연산들은 `Symbol`에서도 제공됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elemental wise multiplication\n",
    "d = a * b\n",
    "# matrix multiplication\n",
    "e = mx.sym.dot(a, b)\n",
    "# reshape\n",
    "f = mx.sym.reshape(d+e, shape=(1,4))\n",
    "# broadcast\n",
    "g = mx.sym.broadcast_to(f, shape=(2,4))\n",
    "# plot\n",
    "mx.viz.plot_network(symbol=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제들에서 정의된 연산 그래프들은 `bind` 함수를 통해서 실제로 사용할 입력 데이터를 적용하여 그래프 연산을 수행할 수 있습니다. 예제는 아래 `데이터 연결 및 수행`에서 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>기본 Neural Networks</font>\n",
    "\n",
    "기본 연산외에, `Symbol`을 사용해서 다양한 Neural Network 래이어들을 정의할 수 있습니다. 다음 예제는 2-layer fully connected neural를 Symbol을 사용해서 정의해봅니다. 이렇게 정의된 neural network는 mx.viz.plot_network를 이용해서 시각화할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mx.sym.Variable('data')\n",
    "net = mx.sym.FullyConnected(data=net, name='fc1', num_hidden=128)\n",
    "net = mx.sym.Activation(data=net, name='relu1', act_type=\"relu\")\n",
    "net = mx.sym.FullyConnected(data=net, name='fc2', num_hidden=10)\n",
    "net = mx.sym.SoftmaxOutput(data=net, name='out')\n",
    "mx.viz.plot_network(net, shape={'data':(100,200)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 symbol은 고유한 문자열로 구성된 이름을 갖습니다. NDArray와 Symbol은 하나의 tensor (다차원 행렬)를 의미하며, **연산자**는 tensor들간의 연산을 정의합니다. 연잔자는 Symbol 또는 NDArray를 입력으로 받으며, hidden neuron 개수 (*num_hidden*)이나 activation type (*act_type*) 등과 같은 **hyperparameter**들을 입력받습니다.\n",
    "\n",
    "Symbol을 여러 argument들을 받는 함수로 생각할 수도 있습니다. 이 argument들은 다음 함수를 통해서 조회할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.list_arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 argument들은 각 symbol에 필요한 파라메터와 입력들로 각 의미는 아래와 같습니다.\n",
    "\n",
    "- *data*: *data* 변수에 필요한 입력 데이터\n",
    "- *fc1_weight*와 *fc1_bias*: 첫번째 fully connected layer *fc1*에서 사용되는 weight와 bias\n",
    "- *fc2_weight*와 *fc2_bias*: 두번째 fully connected layer *fc1*에서 사용되는 weight와 bias\n",
    "- *out_label*: loss에 필요한 label\n",
    "\n",
    "임의로 할당되는 이름 대신에, 각 argument에 이름을 명시적으로 할당하는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mx.symbol.Variable('data')\n",
    "w = mx.symbol.Variable('myweight')\n",
    "net = mx.symbol.FullyConnected(data=net, weight=w, name='fc1', num_hidden=128)\n",
    "net.list_arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>복잡한 조합</font>\n",
    "\n",
    "MXNet은 deep learning에서 일반적으로 사용되는 레이어들에 대한 최적화된 symbol들을 제공합니다. 다음 예제는 두 symbol을 정의하고 element-wise 합을 계산하고, 128개 hidden neuron들을 갖는 fully-connected layer를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = mx.symbol.Variable('data1')\n",
    "rhs = mx.symbol.Variable('data2')\n",
    "net = mx.symbol.FullyConnected(data=lhs + rhs, name='fc1', num_hidden=128)\n",
    "net.list_arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제는 Single forward 조합을 하고 있지만, 보다 복잡한 조합도 가능합니다. 다음 예제는 두개의 Symbol (net1, net2)를 각각 정의하고, net2을 net1에 적용하는 것을 보여줍니다. 이렇게 만들어진 composed symbol은 net1과 net2의 모든 속성들을 갖습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mx.symbol.Variable('data')\n",
    "net1 = mx.symbol.FullyConnected(data=data, name='fc1', num_hidden=10)\n",
    "\n",
    "net2 = mx.symbol.Variable('data2')\n",
    "net2 = mx.symbol.FullyConnected(data=net2, name='fc2', num_hidden=10)\n",
    "\n",
    "composed = net2(data2=net1, name='composed')\n",
    "composed.list_arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='red'>문제</font>\n",
    "위 코드에서 정의한 network (net1, net2, composed)를 mx.viz.plot_network() 함수를 사용해서 시간화 해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx.viz.plot_network(__여기를 채우세요__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "큰 네트워크를 구성하는 경우에는 공통 prefix를 symbol에 적용해서 네트워크 구조를 알기 쉽게할 수 있습니다. 이를 위해서 `mx.name.Prefix` 함수를 아래와 같이 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mx.sym.Variable(\"data\")\n",
    "net = data\n",
    "n_layer = 2\n",
    "for i in range(n_layer):\n",
    "    with mx.name.Prefix(\"layer%d_\" % (i + 1)):\n",
    "        net = mx.sym.FullyConnected(data=net, name=\"fc\", num_hidden=100)\n",
    "net.list_arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>딥러닝 네트워크를 위한 모듈화된 생성자 (Modularized Construction for Deep Networks)</font>\n",
    "\n",
    "Google Inception 네트워크와 같은 Deep 네트워크를 레어어 단위로 구성하는 것은 지루한 작업을 수 있습니다. 이런 경우, 네트워크 생성을 간단하게 하기 위해서 모듈화하기도 합니다.\n",
    "\n",
    "Google Inception 네트워크를 예를 들면, convolution, batch normalization과 rectifiled linear unit (ReLU) activation 레이어들을 한번에 생성하는 factory 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvFactory(data, num_filter, kernel, stride=(1,1), pad=(0, 0),name=None, suffix=''):\n",
    "    conv = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=kernel,\n",
    "                  stride=stride, pad=pad, name='conv_%s%s' %(name, suffix))\n",
    "    bn = mx.sym.BatchNorm(data=conv, name='bn_%s%s' %(name, suffix))\n",
    "    act = mx.sym.Activation(data=bn, act_type='relu', name='relu_%s%s'\n",
    "                  %(name, suffix))\n",
    "    return act\n",
    "prev = mx.sym.Variable(name=\"Previous Output\")\n",
    "conv_comp = ConvFactory(data=prev, num_filter=64, kernel=(7,7), stride=(2, 2))\n",
    "shape = {\"Previous Output\" : (128, 3, 28, 28)}\n",
    "mx.viz.plot_network(symbol=conv_comp, shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로, `ConvFactory` 함수를 이용해서 inception 모듈을 생성하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InceptionFactoryA(data, num_1x1, num_3x3red, num_3x3, num_d3x3red, num_d3x3,\n",
    "                      pool, proj, name):\n",
    "    # 1x1\n",
    "    c1x1 = ConvFactory(data=data, num_filter=num_1x1, kernel=(1, 1), name=('%s_1x1' % name))\n",
    "    # 3x3 reduce + 3x3\n",
    "    c3x3r = ConvFactory(data=data, num_filter=num_3x3red, kernel=(1, 1), name=('%s_3x3' % name), suffix='_reduce')\n",
    "    c3x3 = ConvFactory(data=c3x3r, num_filter=num_3x3, kernel=(3, 3), pad=(1, 1), name=('%s_3x3' % name))\n",
    "    # double 3x3 reduce + double 3x3\n",
    "    cd3x3r = ConvFactory(data=data, num_filter=num_d3x3red, kernel=(1, 1), name=('%s_double_3x3' % name), suffix='_reduce')\n",
    "    cd3x3 = ConvFactory(data=cd3x3r, num_filter=num_d3x3, kernel=(3, 3), pad=(1, 1), name=('%s_double_3x3_0' % name))\n",
    "    cd3x3 = ConvFactory(data=cd3x3, num_filter=num_d3x3, kernel=(3, 3), pad=(1, 1), name=('%s_double_3x3_1' % name))\n",
    "    # pool + proj\n",
    "    pooling = mx.sym.Pooling(data=data, kernel=(3, 3), stride=(1, 1), pad=(1, 1), pool_type=pool, name=('%s_pool_%s_pool' % (pool, name)))\n",
    "    cproj = ConvFactory(data=pooling, num_filter=proj, kernel=(1, 1), name=('%s_proj' %  name))\n",
    "    # concat\n",
    "    concat = mx.sym.Concat(*[c1x1, c3x3, cd3x3, cproj], name='ch_concat_%s_chconcat' % name)\n",
    "    return concat\n",
    "prev = mx.sym.Variable(name=\"Previous Output\")\n",
    "in3a = InceptionFactoryA(prev, 64, 64, 64, 64, 96, \"avg\", 32, name=\"in3a\")\n",
    "mx.viz.plot_network(symbol=in3a, shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 단계로, 전체 네트워크를 여러 inception 모듈들을 연결해서 전체 네트워크를 얻을 수 있습니다. 자세한 내용은 \n",
    "[여기](https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbols/inception-bn.py)를 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>여러 Symbol을 그룹화하기</font>\n",
    "\n",
    "여러 loss layer들을 갖는 neural network를 구성하기 위해서, `mxnet.sym.Group`를 이용해서 symbol들을 그룹화할 수 있습니다. 다음 예는 두개의 output을 그룹화하는 것을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = mx.sym.Variable('data')\n",
    "fc1 = mx.sym.FullyConnected(data=net, name='fc1', num_hidden=128)\n",
    "net = mx.sym.Activation(data=fc1, name='relu1', act_type=\"relu\")\n",
    "out1 = mx.sym.SoftmaxOutput(data=net, name='softmax')\n",
    "out2 = mx.sym.LinearRegressionOutput(data=net, name='regression')\n",
    "group = mx.sym.Group([out1, out2])\n",
    "group.list_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDArray와 관계\n",
    "\n",
    "`NDArray`의 장점:\n",
    "\n",
    "- 직관적임\n",
    "- Native 언어의 기능 잘 동작한다. 예, for loop, if-else 조건문. 그리고, numpy와 같은 라이브러리와도 잘 동작함\n",
    "- 단계별 코딩 및 디버깅이 쉽다\n",
    "\n",
    "`Symbol`의 장점::\n",
    "\n",
    "- NDArray에서 제공하는 거의 모든 기능 제공. (예, `+`, `*`, `sin`, `reshape` 등)\n",
    "- 저장하고, 로딩하고, 시각화하기 쉬움\n",
    "- 백엔드 엔진에 의해서 연산 및 메모리 사용에 대한 최적화가 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol 조작하기\n",
    "\n",
    "`NDArray`와 비교해서 `Symbol`의 가장 중요한 차이점은, 연산을 먼저 정의하고, 데이터를 연산에 할당해서 실행한다는 것입니다. 다음 예제들에서는 Symbol을 직접 조작하는 방법을 소개합니다. 이를 위해서 `module` 패키지를 사용합니다.\n",
    "\n",
    "### <font color='red'>모양(Shape) 와 타입(Type) Inference</font>\n",
    "\n",
    "각 Symbol의 argument, 보조 상태(auxiliary states), 및 output들을 조회할 수 있습니다. input shape과 argument 타입에 따라서, output shape이나 symbol의 타입이 어떻게 되는지 유추해보는 예제는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_name = c.list_arguments()  # get the names of the inputs\n",
    "out_name = c.list_outputs()    # get the names of the outputs\n",
    "# infers output shape given the shape of input arguments\n",
    "arg_shape, out_shape, _ = c.infer_shape(a=(2,3), b=(2,3))\n",
    "# infers output type given the type of input arguments\n",
    "arg_type, out_type, _ = c.infer_type(a='float32', b='float32')\n",
    "print({'input' : dict(zip(arg_name, arg_shape)),'output' : dict(zip(out_name, out_shape))})\n",
    "print({'input' : dict(zip(arg_name, arg_type)),'output' : dict(zip(out_name, out_type))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>데이터 Bind과 수행</font>\n",
    "\n",
    "Symbol `c`는 `a` + `b`를 정의하고 있습니다. 이를 계산하기 위해서는 argument들에 데이터를 바인딩해야 합니다. 바인딩은 `bind` 함수에 device context로 원하는 디바이스(CPU, GPU)를 지정하고 각 변수를 `NDArray`에 매핑하는 `dict`를 전달하고, 그 결과로 executor를 리턴받습니다. executor의 `forward` 함수를 수행해서 연산을 실행하고, `outputs` 속성을 통해서 모든 결과들을 조회합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = c.bind(ctx=mx.cpu(), args={'a' : mx.nd.ones([2,3]),\n",
    "                                'b' : mx.nd.ones([2,3])})\n",
    "ex.forward()\n",
    "print('number of outputs = %d\\nthe first output = \\n%s' % (\n",
    "           len(ex.outputs), ex.outputs[0].asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 Symbol에 다른 데이터를 연결(binding)해서 GPU 자원에서 수행하는 것도 가능합니다.\n",
    "\n",
    "> <font color='red'>문제</font>\n",
    "다음 코드는 CPU에서 수행됩니다. GPU에서 연산이 수행되도록 변경해보세요. (힌트 mx.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_device=mx.cpu()\n",
    "\n",
    "ex_gpu = c.bind(ctx=compute_device, args={'a' : mx.nd.ones([3,4], gpu_device)*2,\n",
    "                                      'b' : mx.nd.ones([3,4], gpu_device)*3})\n",
    "ex_gpu.forward()\n",
    "ex_gpu.outputs[0].asnumpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eval` 함수를 통해서 symbol을 evaluate할 수도 있습니다. `eval` 함수는 `bind` 함수와 `forward` 함수를 함께 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = c.eval(ctx = mx.cpu(), a = mx.nd.ones([2,3]), b = mx.nd.ones([2,3]))\n",
    "print('number of outputs = %d\\nthe first output = \\n%s' % (\n",
    "            len(ex), ex[0].asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network에서 모든 argument 배열을 자동으로 생성해주는 ```simple_bind``` 함수도 많이 사용됩니다. ```simple_bind```를 호출한 후, ```forward```를 호출해서 결과를 계산하거나, 필요한 경우 ```backward``` 함수를 호출해서 gradient를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Load and Save</font>\n",
    "\n",
    "Logically symbols correspond to ndarrays. Symbol와 NDArray는 tensor를 표현하고, 연산의 입력 또는 출력에 사용됩니다. 이후 사용을 위해서, `Symbol` 객체는 `pickle` 또는 `save`/`load` 함수를 이용해서 serialize한 후 파일에 저장하고 다시 읽어올 수 있습니다.\n",
    "\n",
    "`NDArray`를 serialize하면, tensor 데이터가 직렬화되고, 바이너리 형태로 디스크에 저장됩니다.\n",
    "\n",
    "하지만, Symbol은 연산들을 채인으로 구성된 그래프를 표한하고 있습니다. 이 그래프는 output symbol로 표현됩니다. `Symbol`을 직렬화하면, 해당 symbol이 output인 그래프를 직렬화합니다. 직렬화된 내용은 읽기 쉬운 `json` 형식으로 변경되며, `tojson` 함수를 이용해서 `json` 문자열로 변경할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.tojson())\n",
    "c.save('symbol-c.json')\n",
    "c2 = mx.sym.load('symbol-c.json')\n",
    "c.tojson() == c2.tojson()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Symbol\n",
    "\n",
    "`mx.sym.Convolution`이나 `mx.sym.Reshape`와 같은 대부분의 연산은 빠른 속도를 위해서 C++로 작성되어 있습니다. 하지만, Python과 같은 front-end 언어를 이용해서 새로운 연산을 정의할 수 있습니다. 이를 통해서 개발 및 디버깅이 보다 쉬워질 수 있습니다. Python에서 연산을 정의하는 방법은 \n",
    "[How to create new operators](http://mxnet.io/how_to/new_op.html)을 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usages\n",
    "\n",
    "### <font color='blue'>Type Cast</font>\n",
    "\n",
    "기본적으로 MXNet은 32-bit 실수형을 이용합니다. 하지만, 보다 높은 정확도-성능을 위해서 낮은 정확도의 데이터형을 사용할 수 있습니다. 예를 들면, NVidia Tesla Pascal GPU (P100)은 16-bit 실수 연산을 향상시킨 GPU이고, GTX Pascal GPU (GTX 1080)은 8-bit 정수에 최적화되어 있습니다.\n",
    "\n",
    "데이터 타입을 변환하는 방법은 `mx.sym.cast` 함수를 이용하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = mx.sym.Variable('data')\n",
    "b = mx.sym.cast(data=a, dtype='float16')\n",
    "arg, out, _ = b.infer_type(data='float32')\n",
    "print({'input':arg, 'output':out})\n",
    "\n",
    "c = mx.sym.cast(data=a, dtype='uint8')\n",
    "arg, out, _ = c.infer_type(data='int32')\n",
    "print({'input':arg, 'output':out})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Variable Sharing</font>\n",
    "\n",
    "To share the contents between several symbols,\n",
    "we can bind these symbols with the same array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = mx.sym.Variable('a')\n",
    "b = mx.sym.Variable('b')\n",
    "b = a + a * a\n",
    "\n",
    "data = mx.nd.ones((2,3))*2\n",
    "ex = b.bind(ctx=mx.cpu(), args={'a':data, 'b':data})\n",
    "ex.forward()\n",
    "ex.outputs[0].asnumpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- INSERT SOURCE DOWNLOAD BUTTONS -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "display_name": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": ""
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
