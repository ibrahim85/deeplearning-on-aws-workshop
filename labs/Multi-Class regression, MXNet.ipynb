{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass logistic regression\n",
    "\n",
    "아래 예제는 손으로 쓴 숫자를 Image classfication을 통해 구분하는 예제입니다.\n",
    "![png](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/example/mnist.png) \n",
    "Multiclass logistic regression (또는 softmax regression, multinomial regression) 을 통해 숫자를 구분하는 모델을 실습하겠습니다. 필요한 Library를 Import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 모델링 연산을 CPU만을 사용할지, GPU를 활용(mx.gpu(0))할지 선택하는 옵션입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "잘 알려진 Dataset인 MNIST dataset은 28x28 의 흑백으로 손으로 그려진 숫자 이미지를 제공합니다. 0에서 9의 숫자들을로 이루어져 있습니다. 우선은 MXNet 기능을 활용하여 Dataset을 가져옵니다. 가져온 이미지를 float [0,1]로 변환합니다. \n",
    "\n",
    "MNIST dataset detail: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "mnist_train = mx.gluon.data.vision.MNIST(train=True, transform=transform)\n",
    "mnist_test = mx.gluon.data.vision.MNIST(train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset은 Training 용도와 Testing 용도로 구분합니다. 각 데이터 하나는 image와 label를 가집니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image, label = mnist_train[0]\n",
    "print(image.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가로 이 이미지 하나는 (channel, height, width)의 세가지 속성을 가지고, 만약 컬러 이미지라면 channel은 (red, green, and blue) 의 dimension을 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the data and label shapes\n",
    "\n",
    "input 값과 output 값의 개수를 정의합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning 라이브러리는 일반적으로 (batch, channel, height, width)정보를 이미지에서 찾습니다. 그러나, 대부분 시작화를 하기 위한 라이브러리는 (height, width, channel)을 선호합니다. 아래에서 속성을 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = mx.nd.tile(image, (1,1,3))\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변경하고나면 image를 시각화하여 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(im.asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자 5를 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data iterator\n",
    "\n",
    "이제 이미지 데이터들을 iterator에 읽어드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = mx.gluon.data.DataLoader(mnist_train, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test를 위한 이미지 데이터 역시 읽어드립니다. 후에 생성된 모델에 테스트 데이터를 이용하여 검증합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = mx.gluon.data.DataLoader(mnist_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate model parameters\n",
    "\n",
    "이제 모델을 정의합니다. 이번 랩에서는 Multimodal 구조를 사용하지는 않으며, 단순히 각각의 이미지를 single ID vector로 만듭니다. 28x28 = 784 의 경우의 수가 나옵니다. \n",
    "\n",
    "Multiclass classfication 모델을 이용하므로, 입력된 이미지에 대해서 probability 를 추가합니다. 그렇게 하기 위해서는 각 클래스마다 784 wegihts를 가지는 별도의 vector가 하나 필요합니다. 10개의 class가 있으므로, 784 by 10 matrix를 가지게 됩니다.\n",
    "\n",
    "그리고, 각각의 결과마다 하나의 offset을 추가합니다. 이것을 *bias term*으로 정의합니다. 그리고 그것을 10-dimensional array ``b``에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = nd.random_normal(shape=(num_inputs, num_outputs))\n",
    "b = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXNet이 트레이닝 중에 각 파라미터에 대해서 gradients 를 가지도록 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass logistic regression\n",
    "\n",
    "앞서 진행한 Linear regression 랩에서 결과값으로 *yhat*을 정의했었습니다. 그리고 최대한 실제 *y*값에 가깝도록 학습을 하였습니다. 이번에는 *classification*을 목적으로하여, 입력값인 *X*가 특정 *L* 클래스에 할당되는 것을 수행합니다.\n",
    "\n",
    "설명한 것과 같이 기본적인 생각은 입력값 *X*을 10개의 다른 값으로 구분하여 결과값 ``y_linear``에 저장합니다. 그리고 이 결과값을 normalize합니다. 이 normalization 은 결과값이 양수로 처리되며 결과값 yhat으로 맞는 probability로 인식할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    partition =nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_y_linear = nd.random_normal(shape=(2,10))\n",
    "sample_yhat = softmax(sample_y_linear)\n",
    "print(sample_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 결과값의 합이 1인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(nd.sum(sample_yhat, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약간의 차이는 있으나, 정상적으로 동작을 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "이제 모델을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    yhat = softmax(y_linear)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The  cross-entropy loss function\n",
    "\n",
    "트레이닝을 시작하기전에, Loss function을 정의합니다. 여기서는 cross-entropy loss function 을 정의하여 사용합니다. Deep Learning에서 가장 많이 사용되는 Loss function 중 하나입니다. 일반적인 regression 보다 classification이 보다 많이 사용되기 때문입니다. \n",
    "\n",
    "기본적인 접근은 타겟 값인 Y는 10개 중 하나가 1인 vector값을 가집니다. 예를 들어 ``[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]``와 같은 값을 가집니다. 만약 이를 Label 2라 한다면, cross-entropy loss는 Y값이 Label 2에 속하는 값에 대해서만 Probability를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - nd.sum(y * nd.log(yhat), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Linear regression에서 사용한 SGD(Stochastic gradient descent)를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write evaluation loop to calculate accuracy\n",
    "\n",
    "cross-entropy가 유용하긴 하지만 보통 사람들은 정확도를 생각할 때 전체 질문에 몇 개를 맞췄는지에 대해서 Performance를 생각합니다. 따라서 Accuracy를 생각할 떄 아래는 evaluation loop에서 각 Iterator, network, 전체 Dataset에 대해서 계산된 평균 accuracy를 고려합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당연하겠지만, 어떤 입력값이든 10개의 클래스 중에 하나에 포함되게 되므로, 기본적으로 0.10 의 accurary는 가져야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "learning_rate = .001\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "            \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "약 90% 정확도를 확인할 수 있습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
